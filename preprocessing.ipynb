{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO49la4CRWzZ83CJoncuoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HebaRaslan/Arabic-sign-language-Translation/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrnFnhD6PcHu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('arabic')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('Your_Excel_File.xlsx')\n",
        "\n",
        "# Assume that the text is in a column named 'Text'\n",
        "df['Preprocessed_Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "# Save the preprocessed data back to Excel\n",
        "df.to_excel('Preprocessed_Data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('arabic')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('Your_Excel_File.xlsx')\n",
        "\n",
        "# Assume that the text is in a column named 'Text'\n",
        "df['Preprocessed_Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "# Save the preprocessed data back to Excel\n",
        "df.to_excel('Preprocessed_Data.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "SyuGKJEPROg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pandas openpyxl\n",
        "!pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwIsmkWxUkIS",
        "outputId": "e4f401c9-cd48-4e27-b4d2-ca996ff3ea39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
            "Successfully installed emoji-2.12.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 stanza-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('arabic')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Read the Excel files\n",
        "df_gloss = pd.read_excel('/content/sign sentence.xlsx')\n",
        "df_text = pd.read_excel('/content/Target text.xlsx')\n",
        "\n",
        "# Assume that the text is in a column named 'Text'\n",
        "df_gloss['Preprocessed_Text'] = df_gloss['Sentence'].apply(preprocess_text)\n",
        "df_text['Preprocessed_Text'] = df_text['Target Text'].apply(preprocess_text)\n",
        "\n",
        "# Save the preprocessed data back to Excel\n",
        "df_gloss.to_excel('Preprocessed_Gloss_Data.xlsx', index=False)\n",
        "df_text.to_excel('Preprocessed_Text_Data.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmb5P3EvTkAI",
        "outputId": "925a6791-d786-4410-a5e8-06161dea62f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def build_dataset(data_cfg, split):\n",
        "    # Read the Excel files\n",
        "    df_gloss = pd.read_excel('/content/sign sentence.xlsx')\n",
        "    df_text = pd.read_excel('/content/Target text.xlsx')\n",
        "\n",
        "    # Assume that the text is in a column named 'Text'\n",
        "    df_gloss['Preprocessed_Text'] = df_gloss['Sentence'].apply(preprocess_text)\n",
        "    df_text['Preprocessed_Text'] = df_text['Target Text'].apply(preprocess_text)\n",
        "\n",
        "    # Convert the dataframes to dictionaries\n",
        "    gloss_data = df_gloss.to_dict('records')\n",
        "    text_data = df_text.to_dict('records')\n",
        "\n",
        "    # Combine the gloss and text data\n",
        "    data = []\n",
        "    for gloss, text in zip(gloss_data, text_data):\n",
        "        data.append({\n",
        "            'name': gloss['SentenceID'],  # replace 'name' with the actual column name\n",
        "            'gloss': gloss['Preprocessed_Text'],\n",
        "            'text': text['Preprocessed_Text'],\n",
        "        })\n",
        "\n",
        "    # Here you can split the data into training, validation, and test sets if needed\n",
        "    # ...\n",
        "\n",
        "    return data\n",
        "def build_dataloader(cfg, split,\n",
        "    text_tokenizer=None, gloss_tokenizer=None,\n",
        "    mode='auto', val_distributed=False):\n",
        "    dataset = build_dataset(cfg['data'], split)\n",
        "    # ...\n"
      ],
      "metadata": {
        "id": "jKK0cD5iaoAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import pandas as pd\n",
        "import torch\n",
        "from dataset.VideoLoader import load_batch_video\n",
        "from dataset.FeatureLoader import load_batch_feature\n",
        "from dataset.Dataset import build_dataset\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('arabic')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def build_dataset(data_cfg, split):\n",
        "    # Read the Excel files\n",
        "    df_gloss = pd.read_excel('/content/sign sentence.xlsx')\n",
        "    df_text = pd.read_excel('/content/Target text.xlsx')\n",
        "\n",
        "    # Preprocess the text\n",
        "    df_gloss['Preprocessed_Text'] = df_gloss['Sentence'].apply(preprocess_text)\n",
        "    df_text['Preprocessed_Text'] = df_text['Target Text'].apply(preprocess_text)\n",
        "\n",
        "    # Convert the dataframes to dictionaries\n",
        "    gloss_data = df_gloss.to_dict('records')\n",
        "    text_data = df_text.to_dict('records')\n",
        "\n",
        "    # Combine the gloss and text data\n",
        "    data = []\n",
        "    for gloss, text in zip(gloss_data, text_data):\n",
        "        data.append({\n",
        "            'name': gloss['name'],  # replace 'name' with the actual column name\n",
        "            'gloss': gloss['Preprocessed_Text'],\n",
        "            'text': text['Preprocessed_Text'],\n",
        "            'num_frames': gloss['num_frames']  # replace 'num_frames' with the actual column name\n",
        "        })\n",
        "\n",
        "    # Here you can split the data into training, validation, and test sets if needed\n",
        "    # ...\n",
        "\n",
        "    return data\n",
        "\n",
        "def collate_fn_(inputs, data_cfg, task, is_train,\n",
        "    text_tokenizer=None, gloss_tokenizer=None,name2keypoint=None):\n",
        "    outputs = {\n",
        "        'name':[i['name'] for i in inputs],\n",
        "        'gloss':[i.get('gloss','') for i in inputs],\n",
        "        'text':[i.get('text','') for i in inputs],\n",
        "        'num_frames':[i['num_frames'] for i in inputs]}\n",
        "\n",
        "    if task in ['S2T','G2T','S2T_Ensemble']:\n",
        "        tokenized_text = text_tokenizer(input_str=outputs['text'])\n",
        "        outputs['translation_inputs'] = {**tokenized_text}\n",
        "\n",
        "        if task == 'G2T':\n",
        "            tokenized_gloss = gloss_tokenizer(batch_gls_seq=outputs['gloss'])\n",
        "            outputs['translation_inputs']['input_ids'] = tokenized_gloss['input_ids']\n",
        "            outputs['translation_inputs']['attention_mask'] = tokenized_gloss['attention_mask']\n",
        "    return outputs\n",
        "\n",
        "def build_dataloader(cfg, split,\n",
        "    text_tokenizer=None, gloss_tokenizer=None,\n",
        "    mode='auto', val_distributed=False):\n",
        "    dataset = build_dataset(cfg['data'], split)\n",
        "    mode = split if mode=='auto' else mode\n",
        "    if mode=='train':\n",
        "        sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset,\n",
        "            shuffle=cfg['training']['shuffle'] and split=='train'\n",
        "        )\n",
        "    else:\n",
        "        if val_distributed:\n",
        "            sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)\n",
        "        else:\n",
        "            sampler = torch.utils.data.SequentialSampler(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             collate_fn=lambda x:collate_fn_(\n",
        "                                                 inputs=x,\n",
        "                                                 task=cfg['task'],\n",
        "                                                 data_cfg=cfg['data'],\n",
        "                                                 is_train=(mode=='train'),\n",
        "                                                 text_tokenizer=text_tokenizer,\n",
        "                                                 gloss_tokenizer=gloss_tokenizer,\n",
        "                                                 name2keypoint=dataset.name2keypoints),\n",
        "                                             batch_size=cfg['training']['batch_size'],\n",
        "                                             num_workers=cfg['training'].get('num_workers',2),\n",
        "                                             sampler=sampler,\n",
        "                                             )\n",
        "    return dataloader, sampler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "_jj1FOVpBlKT",
        "outputId": "644feaa8-2c43-4dcd-9a0e-f0a7339cee5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a712550be1a1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_batch_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_batch_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "# Download the 'punkt' package from NLTK (contains pre-trained models for tokenization)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is fascinating!\"\n",
        "\n",
        "# Tokenization using NLTK\n",
        "tokens_nltk = word_tokenize(text)\n",
        "\n",
        "# Tokenization using spaCy\n",
        "tokens_spacy = [token.text for token in nlp(text)]\n",
        "\n",
        "print(\"NLTK tokens:\", tokens_nltk)\n",
        "print(\"spaCy tokens:\", tokens_spacy)\n"
      ],
      "metadata": {
        "id": "KZHUcr79IB4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2e4a3d-d010-4d48-fd42-ed79633bcf06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n",
            "spaCy tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
          ]
        }
      ]
    }
  ]
}